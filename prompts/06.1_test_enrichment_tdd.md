# üß™ Phase 3.1: Test Enrichment & TDD Implementation

**Objective**: Implement comprehensive test coverage using Test-Driven Development (TDD) methodology to ensure robust, production-ready functionality.

## üéØ Current State Analysis

Based on code evaluation, the application has:
- ‚úÖ 43 passing tests with good HTTP client coverage
- ‚ùå Missing database integration tests (33% coverage)
- ‚ùå Incomplete core service testing (22% coverage on `fraiseql_service.py`)
- ‚ùå No real database fixtures or test infrastructure
- ‚ùå Missing end-to-end scenarios

## üîÑ TDD Methodology

Follow **RED ‚Üí GREEN ‚Üí REFACTOR** cycle for each feature:

1. **RED**: Write failing test that describes desired behavior
2. **GREEN**: Write minimal code to make test pass
3. **REFACTOR**: Improve code quality while keeping tests green

## üìã Phase 3.1 Tasks

### Task 1: Database Test Infrastructure
**Priority: Critical**

#### 1.1 Create Test Database Setup
```python
# tests/fixtures/database.py
@pytest.fixture(scope="session")
async def test_database():
    """Setup isolated test database with migrations."""
    # RED: Write test that expects clean database
    # GREEN: Implement database creation/cleanup
    # REFACTOR: Optimize setup/teardown performance
```

**TDD Steps**:
1. Write test expecting database tables to exist
2. Implement Alembic migration runner for tests
3. Add automatic cleanup between test sessions
4. Refactor for parallel test execution

#### 1.2 Create Database Session Fixtures
```python
@pytest.fixture
async def db_session():
    """Provide transactional database session."""
    # RED: Test that expects rollback after test
    # GREEN: Implement session with rollback
    # REFACTOR: Add connection pooling optimization
```

### Task 2: Core Service Layer Testing
**Priority: Critical**

#### 2.1 FraiseQLService Comprehensive Testing
**Current Coverage: 22% ‚Üí Target: 90%+**

**TDD Implementation**:

```python
# RED Phase - Write failing tests first:
async def test_create_endpoint_with_validation():
    """Test endpoint creation with full validation."""
    # This should fail initially
    service = FraiseQLService(db_session)
    
    endpoint_data = {
        "name": "test-api",
        "url": "https://api.example.com/graphql",
        "auth_type": "bearer",
        "auth_config": {"token": "test-token"}
    }
    
    result = await service.create_endpoint(endpoint_data)
    
    assert result.success is True
    assert result.endpoint.id is not None
    # Verify in database
    db_endpoint = await service.get_endpoint(result.endpoint.id)
    assert db_endpoint.name == "test-api"

async def test_create_endpoint_duplicate_name_fails():
    """Test that duplicate endpoint names are rejected."""
    # This should fail initially
    service = FraiseQLService(db_session)
    
    # Create first endpoint
    await service.create_endpoint({"name": "test", "url": "https://api1.com"})
    
    # Second with same name should fail
    with pytest.raises(ValidationError, match="already exists"):
        await service.create_endpoint({"name": "test", "url": "https://api2.com"})

async def test_execute_query_with_metrics_tracking():
    """Test query execution with full metrics collection."""
    # This should fail initially
    service = FraiseQLService(db_session)
    
    # Setup endpoint and query
    endpoint = await service.create_endpoint({...})
    query = await service.create_query({...})
    
    # Execute with metrics
    result = await service.execute_query(query.id, endpoint.id, {"var": "value"})
    
    assert result.success is True
    assert result.response_time_ms > 0
    assert result.complexity_score is not None
    
    # Verify metrics were stored
    metrics = await service.get_query_metrics(query.id)
    assert len(metrics) == 1
    assert metrics[0].success is True
```

**GREEN Phase - Implement minimal functionality**:
```python
# src/fraiseql_doctor/services/fraiseql_service.py
class FraiseQLService:
    async def create_endpoint(self, data: dict) -> EndpointResult:
        # Minimal implementation to pass test
        # Validate required fields
        # Check for duplicates
        # Save to database
        # Return result
        
    async def execute_query(self, query_id: UUID, endpoint_id: UUID, variables: dict) -> ExecutionResult:
        # Minimal implementation
        # Load query and endpoint
        # Execute via HTTP client
        # Store metrics
        # Return result
```

**REFACTOR Phase**:
- Extract validation logic
- Optimize database queries
- Add caching layer
- Improve error handling

#### 2.2 Query Management Testing
```python
# RED: Write comprehensive query CRUD tests
async def test_query_lifecycle():
    """Test complete query lifecycle with validation."""
    # Create ‚Üí Read ‚Üí Update ‚Üí Delete with database persistence

async def test_query_syntax_validation():
    """Test GraphQL syntax validation edge cases."""
    # Test various malformed queries

async def test_query_variable_extraction():
    """Test variable extraction from complex queries."""
    # Test nested variables, arrays, etc.
```

#### 2.3 Health Monitoring Testing
```python
# RED: Write health check tests
async def test_endpoint_health_monitoring():
    """Test continuous health monitoring functionality."""
    
async def test_health_check_failure_detection():
    """Test detection and alerting of endpoint failures."""
    
async def test_health_metrics_aggregation():
    """Test health metrics collection and reporting."""
```

### Task 3: Integration & End-to-End Testing
**Priority: High**

#### 3.1 Full Stack Integration Tests
```python
# RED: Write end-to-end scenario tests
async def test_complete_monitoring_workflow():
    """Test: Create endpoint ‚Üí Add queries ‚Üí Schedule monitoring ‚Üí Collect results."""
    
async def test_multi_endpoint_query_execution():
    """Test: Execute same query across multiple endpoints concurrently."""
    
async def test_failure_recovery_workflow():
    """Test: Handle endpoint failures ‚Üí Retry ‚Üí Circuit breaker ‚Üí Recovery."""
```

#### 3.2 Performance & Load Testing
```python
# RED: Write performance requirement tests
async def test_concurrent_query_execution():
    """Test handling 50+ concurrent queries."""
    
async def test_large_result_set_handling():
    """Test handling large GraphQL responses."""
    
async def test_database_connection_pooling():
    """Test database performance under load."""
```

### Task 4: Error Handling & Edge Cases
**Priority: High**

#### 4.1 Comprehensive Error Scenarios
```python
# RED: Write error handling tests
async def test_network_failure_handling():
    """Test handling of various network failures."""
    
async def test_malformed_graphql_response():
    """Test handling of invalid GraphQL responses."""
    
async def test_database_connection_loss():
    """Test recovery from database connection issues."""
    
async def test_authentication_expiry():
    """Test handling of expired authentication tokens."""
```

#### 4.2 Data Validation & Constraints
```python
# RED: Write validation tests
async def test_endpoint_url_validation():
    """Test URL format validation and normalization."""
    
async def test_auth_config_validation():
    """Test authentication configuration validation."""
    
async def test_query_complexity_limits():
    """Test query complexity validation and rejection."""
```

### Task 5: CLI Integration Testing
**Priority: Medium**

#### 5.1 Command Integration Tests
```python
# RED: Write CLI integration tests
async def test_cli_endpoint_management():
    """Test CLI commands for endpoint CRUD operations."""
    
async def test_cli_query_execution():
    """Test CLI query execution with real database."""
    
async def test_cli_monitoring_commands():
    """Test CLI monitoring and reporting commands."""
```

## üõ†Ô∏è Implementation Strategy

### Week 1: Database Foundation
- Day 1-2: Database test infrastructure
- Day 3-4: Basic CRUD operation tests
- Day 5: Performance optimization

### Week 2: Core Services
- Day 1-3: FraiseQLService comprehensive testing
- Day 4-5: Query management and health monitoring

### Week 3: Integration & Reliability
- Day 1-2: End-to-end integration tests
- Day 3-4: Error handling and edge cases
- Day 5: Performance and load testing

### Week 4: Polish & Documentation
- Day 1-2: CLI integration testing
- Day 3-4: Test documentation and examples
- Day 5: Final optimization and refactoring

## üìä Success Criteria

### Code Coverage Targets
- **Overall**: 85%+ (from current ~60%)
- **Core Services**: 90%+ (from current 22%)
- **Database Layer**: 80%+ (from current 33%)
- **Models**: 95%+ (maintain current 86%+)

### Test Quality Metrics
- **No Mocking** of core business logic
- **Real Database** operations in all integration tests
- **Realistic Scenarios** covering production use cases
- **Performance Benchmarks** with actual measurements

### Functional Requirements
- All database operations tested with real transactions
- Error scenarios tested with actual failure conditions
- Authentication tested with real token validation
- Concurrency tested with actual parallel execution

## üîß Tools & Techniques

### Test Infrastructure
```python
# Use these patterns consistently:

# Database testing with real transactions
@pytest.fixture
async def db_transaction():
    async with test_db.begin() as trans:
        yield trans
        await trans.rollback()  # Clean slate for each test

# HTTP testing with realistic mocks
@pytest.fixture
def graphql_endpoint_mock():
    with aioresponses() as m:
        # Setup realistic GraphQL responses
        yield m

# Performance testing
import pytest_benchmark
def test_query_performance(benchmark):
    result = benchmark(execute_complex_query)
    assert result.response_time_ms < 1000
```

### TDD Anti-Patterns to Avoid
- ‚ùå Writing tests after implementation
- ‚ùå Mocking core business logic
- ‚ùå Testing implementation details instead of behavior
- ‚ùå Skipping the RED phase
- ‚ùå Not refactoring after GREEN phase

### TDD Best Practices
- ‚úÖ Start with simplest failing test
- ‚úÖ Write only enough code to pass the test
- ‚úÖ Refactor mercilessly while keeping tests green
- ‚úÖ Use descriptive test names that document behavior
- ‚úÖ Test edge cases and error conditions

## üìö Expected Deliverables

1. **Test Infrastructure**: Complete database and session management for tests
2. **Service Layer Tests**: Comprehensive coverage of all business logic
3. **Integration Tests**: End-to-end scenarios covering real-world usage
4. **Performance Tests**: Benchmarks and load testing
5. **Error Handling Tests**: Comprehensive failure scenario coverage
6. **Documentation**: Test strategy and maintenance guide

## üéØ Final Validation

Before phase completion, verify:
- [ ] All tests use real database operations (no ORM mocking)
- [ ] Core services achieve 90%+ test coverage
- [ ] Integration tests cover complete user workflows
- [ ] Performance tests establish baseline metrics
- [ ] Error scenarios are comprehensively tested
- [ ] Test suite runs reliably in CI/CD environment

This TDD approach ensures that every line of production code is driven by failing tests, resulting in robust, well-designed functionality that meets real-world requirements.